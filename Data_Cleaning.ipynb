{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afff71ea",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "<b>Contents</b>\n",
    "\n",
    "2. [Introduction](#2) <br>\n",
    "    2.1. [Unpivoting dataframe](#2.1) <br>\n",
    "    2.2. [Merge Volume, Speed and Occupancy](#2.2) <br>\n",
    "    2.3. [Traditional missing data handling: Interpolation() and fillna()](#2.3) <br>\n",
    "    2.4. [RNN time series imputation](#2.4) <br>\n",
    "    2.5. [Merge incident type to Link All Days data by ID and DateTime](#2.5) \n",
    "\n",
    "<b>Input:</b>\n",
    "\n",
    "- Smart Freeways Data 2018 - Links All Days.xlsx\n",
    "\n",
    "14 Links' per minute time series data from 1 Jan 2018 to 25 Oct 2018\n",
    "\n",
    "       - Sheet 1: NPI Links - Volume    (data available up to 1 Nov 18)\n",
    "       - Sheet 2: NPI Links - Speed     (data available up to 25 Oct 18)\n",
    "       - Sheet 3: NPI Links - Occupancy (data available up to 25 Oct 18)\n",
    "    \n",
    "- [Minor_Incidents_Log2.xlsx](data/Minor_Incidents_Log2.xlsx)\n",
    "\n",
    "Incident Data on Kwinana Fwy from 1 Jan 2018 to 30 Nov 2018)\n",
    "\n",
    "<b>Output:</b>\n",
    "\n",
    "- [LAD.csv](data/LAD.csv)\n",
    "\n",
    "The new dataframe consists of 298 days $\\times$ 1440 mins $\\times$ 14 links = 6007680 rows\n",
    "\n",
    "|ID|DateTime|Length|Volume|Speed|Occupancy|\n",
    "|--|--------|------|------|-----|---------|\n",
    "|int|datetimens[64]|float32|int|float32|float32|\n",
    "\n",
    "- [LAD+incident.csv](data/LAD+incident.csv)\n",
    "|ID|DateTime|Length|Volume|Speed|Occupancy|Incident_type|\n",
    "|--|--------|------|------|-----|---------|--------|\n",
    "|int|datetimens[64]|float32|int|float32|float32|object|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5b9d8",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "The <b>Smart Freeways Data 2018 - Links All Days.xlsx</b> file contains per minute data from Kwinana Freeway (Inner) Northbound on 14 NPI links, from 1 January to 25 October 2018. The excel file consists of three spreadsheets of 299 columns and 20515 rows, each summarising speed, volume and occupancy in pivot table layout. Each column represents date in `DD Mon YY` format, whereas each row represents time in `HH:MM` format, respectively. In addition, the length of each NPI link (in metre) is given. \n",
    "\n",
    "To facilitate the data exploration and analysis, the data frame is reshaped into long format so that each row is one time point per attribute, and the three spreadsheets are merged on timestamps and NPI Link. The new dataframe then consists of 6007680 rows (298 days $\\times$ 1440 mins $\\times$ 14 NPI links) and 6 columns, namely `NPI Link name`, `DateTime`, `Length`, `Volume`, `Speed` and `Occupancy`.\n",
    "\n",
    "For accessibility of each NPI Link data, the new column, `ID`, is introduced to encode NPI Link name with values between 1 and 14. The mapping of a label encoder and the sample data frame structure are shown as follows:\n",
    "\n",
    "| ID | Link Description |\n",
    "|---------|---------------|\n",
    "|1|Kwinana Fwy NB between Kwinana Fwy Nth Bnd H015 Nth Bnd - H018 East Bnd & Kwinana Fwy Nth Bnd H018 W|\n",
    "|2|Kwinana Fwy NB between Kwinana Fwy Nth Bnd H018 West Bnd - H015 Nth Bnd & Farrington Rd On - H015 Nt|\n",
    "|3|Kwinana Fwy NB between Farrington Rd On - H015 Nth Bo & H015 Nth Bound - South St Off|\n",
    "|4|Kwinana Fwy NB between H015 Nth Bound - South St Off & South St On - H015 Nth Bound|\n",
    "|5|Kwinana Fwy NB between South St On - H015 Nth Bound & H015 Nth Bound - Leach Hwy Off|\n",
    "|6|Kwinana Fwy NB between H015 Nth Bound - Leach Hwy Off & Leach Hwy West Bound On - H015|\n",
    "|7|Kwinana Fwy NB between Leach Hwy West Bound On - H015 & Leach Hwy East Bound On - H015|\n",
    "|8|Kwinana Fwy NB between Leach Hwy East Bound On - H015 & Cranford Av On - H015 Nth Bou|\n",
    "|9|Kwinana Fwy NB between Cranford Av On - H015 Nth Bou & H015 Sth Bound - H548|\n",
    "|10|Kwinana Fwy NB between H015 Sth Bound - H548 & Manning Rd - H547 On Kwinana Fwy Nth Bound|\n",
    "|11|Kwinana Fwy NWB between Manning Rd - H547 On Kwinana Fwy Nth Bound & Canning Hwy - H549 On Kwinana F|\n",
    "|12|Kwinana Fwy NB between Kwinana Fwy (northbound) Bus Ln From Canning Hwy: H013 On To H015 Northbound|\n",
    "|13|Kwinana Fwy NB between Mill Pt Rd - H500 On Kwinana Fwy Nth Bound & Kwinana Fwy Nth Bound H503 Off -|\n",
    "|14|Kwinana Fwy NB between Kwinana Fwy Nth Bound H503 Off - Mill Pt Rd & Mitchell Fwy Nth Bound|\n",
    "\n",
    "\n",
    "\n",
    "|ID|DateTime|Length|Volume|Speed|Occupancy|\n",
    "|--|--------|------|------|-----|---------|\n",
    "|int|datetimens[64]|float32|int|float32|float32|\n",
    "\n",
    "Prior to feeding the data to conduct simulation or perform deep learning tasks, it is essential to perform an exploratory data analysis to determine the necessity of data engineering and to get insightful information about data features. Time series data such as traffic information are known to suffer from missing values problem due to various unexpected incidents, e.g. broken sensors or faulty detectors. These incomplete time series data are not only difficult to utilise, they also affect the performance of simulation and forecasting tasks. Therefore, time series data imputation has to be carried out with utmost care.\n",
    "\n",
    "\n",
    "In this section, data cleaning is conducted alongside with the study of missing values distribution.\n",
    "\n",
    "\n",
    "\n",
    "From quick data inspection, there are approximately 0.5% missing data from each spreadsheet, including the date 25th and 26th of March 2018. Traditionally, these data points with missing values can either be ignored altogether or fill in with median or mean values. The proposed traditional method for the data frame is to fill in the small data gap with less than 5 consecutive missing rows using linear interpolation, whereas the large data gap with more than or equal to 5 consecutive missing rows are filled in using the average values grouped by NPI link name, day of week and time.\n",
    "\n",
    "\n",
    "Although, the traditional method seems to be sufficient, there is still a risk of losing essential information from the data due to oversummarising. Recently, deep learning models such as recurrent neural networks (RNN) have gained great many attentions in multivariate time series imputation as they are known to best captures time information from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d1a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:07:13.874783Z",
     "start_time": "2022-01-18T07:07:09.862372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from datetime import date\n",
    "import holidays\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "mpl.rcParams[\"font.sans-serif\"] = \"Verdana\"\n",
    "mpl.rcParams[\"lines.markersize\"] = 20\n",
    "# mpl.rc('axes', labelsize=14)\n",
    "# mpl.rc('xtick', labelsize=12)\n",
    "# mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6ca56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:09:28.895690Z",
     "start_time": "2022-01-18T07:07:13.891736Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/Smart Freeways Data 2018 - Links All Days.xlsx\",\n",
    "                   sheet_name='NPI Links - Volume', skiprows=2, header=0)\n",
    "df2 = pd.read_excel(\"data/Smart Freeways Data 2018 - Links All Days.xlsx\",\n",
    "                   sheet_name='NPI Links - Speed', skiprows=2, header=0)\n",
    "df3 = pd.read_excel(\"data/Smart Freeways Data 2018 - Links All Days.xlsx\",\n",
    "                   sheet_name='NPI Links - Occupancy', skiprows=2, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a325fd4",
   "metadata": {},
   "source": [
    "## Unpivoting dataframe <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa201ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:09:29.023200Z",
     "start_time": "2022-01-18T07:09:29.008238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Restructure columns so that we have all the dates\n",
    "cols1 = df.columns[:3]\n",
    "cols2 = [d.strftime('%d %b %y') for d in pd.date_range(start='2018-01-01',\n",
    "                                               end='2018-10-25',\n",
    "                                               freq='D')]\n",
    "cols = [*cols1, *cols2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15d091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:11:01.415476Z",
     "start_time": "2022-01-18T07:09:29.072096Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpivot_data(df):\n",
    "    \"\"\"\n",
    "    Function to add missing date columns and unpivot\n",
    "    Smart Freeways Data 2018 - Links All Days.xlsx data file\n",
    "    \"\"\"\n",
    "    # Missing Row Labels and NPI Link Length filled \n",
    "    df[['Row Labels', 'NPI Link Length']] = df[['Row Labels', 'NPI Link Length']].fillna(method='ffill')\n",
    "    \n",
    "    # Keep rows that have %H:%M time format\n",
    "    df = df[df['Hour Formatted'].notna()]\n",
    "    df = df[df['Hour Formatted'].str.len() <= 5]\n",
    "    \n",
    "    # Add 25 Mar 18 and/or 26 Mar 18 if one of either of them do not exist\n",
    "    df = df.reindex(df.columns.union(cols, sort=False), axis=1)\n",
    "    \n",
    "    # Replace 24-27 Mar with 17-20 Mar\n",
    "    # df['24 Mar 18'] = df['17 Mar 18']\n",
    "    # df['25 Mar 18'] = df['18 Mar 18']\n",
    "    # df['26 Mar 18'] = df['19 Mar 18']\n",
    "    # df['27 Mar 18'] = df['20 Mar 18']\n",
    "    \n",
    "    # Change date columns' format from %d %b %y to %Y-%m-%d datetime\n",
    "    aslist = df.columns.tolist()\n",
    "    aslist[3:] = pd.to_datetime(df.columns[3:])\n",
    "    df.columns = aslist\n",
    "    \n",
    "    # Change Hour Formatted column to datetime object\n",
    "    df['Hour Formatted'] = pd.to_datetime(df['Hour Formatted'],\n",
    "                                              format='%H:%M')\n",
    "    \n",
    "    # Melt (unpivot) dataframe\n",
    "    df = df.melt(id_vars=['Row Labels', 'NPI Link Length', 'Hour Formatted'],\n",
    "                  var_name='DateTime', value_name='Value')\n",
    "    # Create DateTime column from date columns and HourFormatted\n",
    "    df['DateTime'] = (pd.to_datetime(df['DateTime']) +  \n",
    "                       pd.to_timedelta(df.pop('Hour Formatted')\n",
    "                                       .dt.strftime('%H:%M:%S')))\n",
    "    # Sort dataframe by Row Labels and DateTime\n",
    "    df = df.sort_values(['Row Labels', 'DateTime'], \n",
    "                          ascending=[True, True])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "vol_df = unpivot_data(df)  # 1 Jan 2018 - 1 Nov 2018\n",
    "spe_df = unpivot_data(df2) # 1 Jan 2018 - 25 Oct 2018\n",
    "occ_df = unpivot_data(df3) # 1 Jan 2018 - 25 Oct 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498e9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:11:01.511133Z",
     "start_time": "2022-01-18T07:11:01.495710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "vol_df.rename({'Row Labels': 'Link',\n",
    "               'NPI Link Length': 'Length',\n",
    "               'Value': 'Volume'}, axis=1, inplace=True)\n",
    "spe_df.rename({'Row Labels': 'Link',\n",
    "               'NPI Link Length': 'Length',\n",
    "               'Value': 'Speed'}, axis=1, inplace=True)\n",
    "occ_df.rename({'Row Labels': 'Link',\n",
    "               'NPI Link Length': 'Length',\n",
    "               'Value': 'Occupancy'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefefc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:11:01.576334Z",
     "start_time": "2022-01-18T07:11:01.562540Z"
    }
   },
   "outputs": [],
   "source": [
    "vol_df # 305 days * 1440 mins * 14 Links = 6148800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13652a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:11:01.638812Z",
     "start_time": "2022-01-18T07:11:01.623704Z"
    }
   },
   "outputs": [],
   "source": [
    "spe_df # 298 days * 1440 mins * 14 Links = 6007680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c2e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:11:01.701528Z",
     "start_time": "2022-01-18T07:11:01.687533Z"
    }
   },
   "outputs": [],
   "source": [
    "occ_df # 298 days * 1440 mins * 14 Links = 6007680"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1c566",
   "metadata": {},
   "source": [
    "## Merge Volume, Speed and Occupancy <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c9d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:11:07.505854Z",
     "start_time": "2022-01-18T07:11:01.749370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge volume and speed\n",
    "dataset = vol_df.merge(spe_df, on=['Link','Length','DateTime'])\n",
    "\n",
    "# Merge dataset and occupancy\n",
    "dataset = dataset.merge(occ_df, on=['Link','Length','DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6fefc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:17:49.731103Z",
     "start_time": "2022-01-18T07:17:47.708418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map Link name to ID for convenience sake\n",
    "dataset['ID'] = dataset['Link']\n",
    "dataset = dataset.replace({'ID':{\n",
    "    'Kwinana Fwy NB between Kwinana Fwy Nth Bnd H015 Nth Bnd - H018 East Bnd & Kwinana Fwy Nth Bnd H018 W': 1,\n",
    "    'Kwinana Fwy NB between Kwinana Fwy Nth Bnd H018 West Bnd - H015 Nth Bnd & Farrington Rd On - H015 Nt': 2,\n",
    "    'Kwinana Fwy NB between Farrington Rd On - H015 Nth Bo & H015 Nth Bound - South St Off': 3,\n",
    "    'Kwinana Fwy NB between H015 Nth Bound - South St Off & South St On - H015 Nth Bound': 4,\n",
    "    'Kwinana Fwy NB between South St On - H015 Nth Bound & H015 Nth Bound - Leach Hwy Off': 5,\n",
    "    'Kwinana Fwy NB between H015 Nth Bound - Leach Hwy Off & Leach Hwy West Bound On - H015': 6,\n",
    "    'Kwinana Fwy NB between Leach Hwy West Bound On - H015 & Leach Hwy East Bound On - H015': 7,\n",
    "    'Kwinana Fwy NB between Leach Hwy East Bound On - H015 & Cranford Av On - H015 Nth Bou': 8,\n",
    "    'Kwinana Fwy NB between Cranford Av On - H015 Nth Bou & H015 Sth Bound - H548': 9,\n",
    "    'Kwinana Fwy NB between H015 Sth Bound - H548 & Manning Rd - H547 On Kwinana Fwy Nth Bound': 10,\n",
    "    'Kwinana Fwy NWB between Manning Rd - H547 On Kwinana Fwy Nth Bound & Canning Hwy - H549 On Kwinana F': 11,\n",
    "    'Kwinana Fwy NB between Kwinana Fwy (northbound) Bus Ln From Canning Hwy: H013 On To H015 Northbound': 12,\n",
    "    'Kwinana Fwy NB between Mill Pt Rd - H500 On Kwinana Fwy Nth Bound & Kwinana Fwy Nth Bound H503 Off -': 13,\n",
    "    'Kwinana Fwy NB between Kwinana Fwy Nth Bound H503 Off - Mill Pt Rd & Mitchell Fwy Nth Bound': 14,\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3e85c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:18:09.120195Z",
     "start_time": "2022-01-18T07:18:07.886638Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.groupby(['ID','Link','Length']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd9bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T02:32:03.144483Z",
     "start_time": "2022-01-18T02:32:03.006450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "dataset = dataset[['DateTime', 'ID', 'Link', 'Length',\n",
    "                   'Volume', 'Speed', 'Occupancy']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff81607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T02:32:06.397297Z",
     "start_time": "2022-01-18T02:32:05.845491Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae13db",
   "metadata": {},
   "source": [
    "# Visualisation  <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47728809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T11:26:48.569664Z",
     "start_time": "2022-01-17T11:26:48.464948Z"
    }
   },
   "outputs": [],
   "source": [
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd3b5a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T11:26:50.527552Z",
     "start_time": "2022-01-17T11:26:50.194480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change 0s in Volume column to NaNs\n",
    "df['Volume'].replace(0, np.nan, inplace=True)\n",
    "df['DateTime'] = df['DateTime'].astype('int64')\n",
    "\n",
    "\n",
    "df = df[['Volume', 'Speed', 'Occupancy']].values\n",
    "\n",
    "# Take rows with NaN's only\n",
    "X = df[ np.isnan(df).any(axis=1)]\n",
    "\n",
    "# Full trends\n",
    "F = df[ ~np.isnan(df).any(axis=1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850c4f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T11:33:27.976501Z",
     "start_time": "2022-01-17T11:33:27.312318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of consecutive NaNs in Volume column\n",
    "# (dataset.Volume.isnull().astype(int)\n",
    "#              .groupby(dataset.Volume.notnull().astype(int).cumsum()).sum()\n",
    "#              .value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f1822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T15:47:29.469032Z",
     "start_time": "2022-01-17T15:47:29.457029Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa840e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T16:06:16.977625Z",
     "start_time": "2022-01-17T16:06:16.655489Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap=sns.cubehelix_palette(start=2, rot=0, dark=0, light=.95,\n",
    "                           reverse=True, as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(np.isnan(X), xticklabels=['Volume', 'Speed', 'Occupancy'],\n",
    "            cmap=cmap)\n",
    "plt.title('NaN distribution')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf2586",
   "metadata": {},
   "source": [
    "The heatmap plot shows the distribution of missing values (NaNs) from each features. It appears that most of the NaNs are collectively missing from volume and speed data, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa072909",
   "metadata": {},
   "source": [
    "# Data Imputation <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f7279",
   "metadata": {},
   "source": [
    "## Traditional: Interpolation() and fillna() <a class=\"anchor\" id=\"4.1\"></a>\n",
    "\n",
    "- Assume that volume will never be zero at all time (at least 1), so 0s are replaced with NaNs.\n",
    "- For small missing data gap (<5 consecutive NaNs), interpolate/extrapolate with hard limit = 5\n",
    "- For large missing data gap (>=5 consecutive NaNs), fill NaNs with average (mean) values grouped by ID, day of week and timestamp.\n",
    "- Round Volume and Occupancy values to integer and to 1 decimal place, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 0s in Volume column to NaNs\n",
    "dataset['Volume'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of consecutive NaNs\n",
    "vol_check = (dataset.Volume.isnull().astype(int)\n",
    "             .groupby(dataset.Volume.notnull().astype(int).cumsum()).sum())\n",
    "vol_check.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiindex dataframe\n",
    "dat = dataset.set_index(['ID', 'DateTime'])\n",
    "dat = dat.sort_index()\n",
    "\n",
    "# Retrieve only Length, Volume, Speed, and Occupancy\n",
    "dat = dat.iloc[:,-4:]; dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1b74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For small missing data gap (<5 consecutive NaNs),\n",
    "# interpolate/extrapolate with hard limit = 5\n",
    "mask = dat.copy()\n",
    "grp = ((mask.notnull() != mask.shift().notnull()).cumsum())\n",
    "grp['ones'] = 1\n",
    "for i in dat.columns:\n",
    "    mask[i] = (grp.groupby(i)['ones'].transform('count') < 5) | dat[i].notnull()\n",
    "\n",
    "dat = dat.interpolate().bfill()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only >=5 consecutive NaNs left\n",
    "(dat.Volume.isnull().astype(int)\n",
    " .groupby(dat.Volume.notnull().astype(int).cumsum()).sum()\n",
    " .value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check average (mean) of values grouped by ID, day of week and timestamp\n",
    "(dat.groupby([dat.index.get_level_values(0), \n",
    "             dat.index.get_level_values(1).strftime('%A %H:%M')])\n",
    " [['Volume','Speed','Occupancy']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For big missing data gap (>=5 consecutive NaNs),\n",
    "# fill with average (mean) of the same day of week and time\n",
    "# Group by index 0 and index 1 (ID and day of week timestamp)\n",
    "dat[['Volume','Speed','Occupancy']] = (dat[['Volume','Speed','Occupancy']]\n",
    "    .fillna(dat.groupby([dat.index.get_level_values(0), \n",
    "                         dat.index.get_level_values(1).strftime('%A %H:%M')])\n",
    "           [['Volume','Speed','Occupancy']].transform('mean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of consecutive NaNs for Volume\n",
    "(dat.Volume.isnull().astype(int)\n",
    " .groupby(dat.Volume.notnull().astype(int).cumsum()).sum()\n",
    " .value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de563d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of consecutive NaNs for Speed\n",
    "(dat.Speed.isnull().astype(int)\n",
    " .groupby(dat.Speed.notnull().astype(int).cumsum()).sum()\n",
    " .value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of consecutive NaNs for Occupancy\n",
    "(dat.Occupancy.isnull().astype(int)\n",
    " .groupby(dat.Occupancy.notnull().astype(int).cumsum()).sum()\n",
    " .value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99891f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['Volume'] = round(dat['Volume']) # round to nearest integer\n",
    "dat['Occupancy'] = round(dat['Occupancy']) # round to 1 dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as CSV\n",
    "dat.to_csv('data/LAD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9470b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LAD file if have not run from the beginning\n",
    "df = pd.read_csv(\"data/LAD.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a53d2c",
   "metadata": {},
   "source": [
    "## RNN Time Series imputation <a class=\"anchor\" id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dc60f",
   "metadata": {},
   "source": [
    "##  Merge incident type to Link All Days data by ID and nearest DateTime <a class=\"anchor\" id=\"2.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Incident file\n",
    "inc_df = pd.read_excel(\"data/Minor_Incidents_Log2.xlsx\",\n",
    "                   sheet_name='Sheet1', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc17f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop crossRoadRef and rearrange columns\n",
    "inc_df = inc_df[['ID', 'DateTime', 'Incident_type']]\n",
    "inc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe014656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round DateTime timestamp to the nearest minute\n",
    "inc_df.DateTime = inc_df.DateTime.round('min')\n",
    "inc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b52cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether we have duplicate entries\n",
    "# We does!\n",
    "inc_df[inc_df.duplicated(subset=['ID','DateTime'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb02422",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df.iloc[271,1] = inc_df.iloc[271,1].replace(minute=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad179788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again for duplicate ID and time\n",
    "inc_df[inc_df.duplicated(subset=['ID','DateTime'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DateTime = pd.to_datetime(df.DateTime)\n",
    "inc_df.DateTime = pd.to_datetime(inc_df.DateTime)\n",
    "inc_df.sort_values('DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058818cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f092a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = (pd.merge(df.sort_values(['ID', 'DateTime']),\n",
    "                      inc_df.sort_values(['ID', 'DateTime']),\n",
    "                      how='left',\n",
    "                      on=['ID', 'DateTime'])\n",
    "             .sort_values(['ID', 'DateTime'])\n",
    "             .reset_index()\n",
    "            )\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.Incident_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as CSV\n",
    "df_merged.to_csv('data/LAD+incident.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f9aee",
   "metadata": {},
   "source": [
    "# Visualisation <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f204cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LAD+incident file if have not run from the beginning\n",
    "df = pd.read_csv(\"data/LAD+incident.csv\", header=0)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
